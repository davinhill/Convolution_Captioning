{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocess_Captions",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davinhill/Convolution_Captioning/blob/master/Preprocess_Captions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P7oEpN54c_M",
        "colab_type": "code",
        "outputId": "94db6324-aa21-43a9-f47d-8eaf86d791a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7PNBFMtIog8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# Path where the data should be located\n",
        "path = '/content/drive/My Drive/Colab Notebooks/IE534_ImageCaptioning/Data'\n",
        "os.chdir(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6LjW-OkIdlY",
        "colab_type": "code",
        "outputId": "5aa56386-5f61-4d15-e30a-cf654f89669a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from pycocotools.coco import COCO \n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import itertools\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDZw47qQ9yx5",
        "colab_type": "code",
        "outputId": "58c594cf-0be2-4794-ef2e-f01df4ef0b61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Load Captions from Annotation File\n",
        "\n",
        "##################\n",
        "# Validation Set\n",
        "##################\n",
        "# Load Annotations\n",
        "cap = COCO(os.path.join(path, 'annotations/captions_val2017.json'))\n",
        "\n",
        "# Create list of captions\n",
        "ann_ids = cap.getAnnIds(imgIds = [])\n",
        "ann_list = cap.loadAnns(ids = ann_ids)\n",
        "\n",
        "cap_val_raw = []\n",
        "for dict in ann_list:\n",
        "  cap_val_raw.append(dict['caption'])\n",
        "\n",
        "##################\n",
        "# Train Set\n",
        "##################\n",
        "# Load Annotations\n",
        "cap = COCO(os.path.join(path, 'annotations/captions_train2017.json'))\n",
        "\n",
        "# Create list of captions\n",
        "ann_ids = cap.getAnnIds(imgIds = [])\n",
        "ann_list = cap.loadAnns(ids = ann_ids)\n",
        "\n",
        "cap_train_raw = []\n",
        "for dict in ann_list:\n",
        "  cap_train_raw.append(dict['caption'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.06s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.97s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sqlziIH_XLE",
        "colab_type": "code",
        "outputId": "d1783712-54bd-4d2d-ad8a-94d2d8ac0872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Number of Captions:\n",
        "len(cap_train_raw)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "591753"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAAgqRcKKALn",
        "colab_type": "code",
        "outputId": "11da865e-4e44-4f1a-f1e1-03e8f8639b2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Tokenize Caption List\n",
        "\n",
        "cap_val = []\n",
        "for caption in cap_val_raw:\n",
        "  line = nltk.word_tokenize(caption)\n",
        "  line = [w.lower() for w in line] \n",
        "  cap_val.append(line)\n",
        "\n",
        "\n",
        "cap_train = []\n",
        "for caption in cap_train_raw:\n",
        "  line = nltk.word_tokenize(caption)\n",
        "  line = [w.lower() for w in line] \n",
        "  cap_train.append(line)\n",
        "\n",
        "\n",
        "# Number of Tokens\n",
        "no_of_tokens = []\n",
        "for tokens in cap_train:\n",
        "    no_of_tokens.append(len(tokens))\n",
        "no_of_tokens = np.asarray(no_of_tokens)\n",
        "print('Total: ', np.sum(no_of_tokens), ' Min: ', np.min(no_of_tokens), ' Max: ', np.max(no_of_tokens), ' Mean: ', np.mean(no_of_tokens), ' Std: ', np.std(no_of_tokens), ' Med: ', np.median(no_of_tokens))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total:  6687792  Min:  6  Max:  57  Mean:  11.301661335050266  Std:  2.596305429474608  Med:  11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rycAXy5X_tpF",
        "colab_type": "code",
        "outputId": "4409a497-39ce-4d83-e69b-d280f9349a6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# create word_to_id and id_to_word translations\n",
        "# word_to_id is a dictionary\n",
        "# id_to_word is a np array\n",
        "\n",
        "all_tokens = itertools.chain.from_iterable(cap_train)\n",
        "word_to_id = {token: idx for idx, token in enumerate(set(all_tokens))}\n",
        "\n",
        "\n",
        "all_tokens = itertools.chain.from_iterable(cap_train)\n",
        "id_to_word = [token for idx, token in enumerate(set(all_tokens))]\n",
        "\n",
        "id_to_word = np.asarray(id_to_word)\n",
        "\n",
        "\n",
        "print(len(id_to_word), \"unique words / tokens\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "29556 unique words / tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgI8rBpZBDl7",
        "colab_type": "code",
        "outputId": "d3266f86-6d96-44fc-df00-a8b65d4fde42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "## let's sort the indices by word frequency instead of random\n",
        "x_train_token_ids = [[word_to_id[token] for token in x] for x in cap_train]\n",
        "count = np.zeros(id_to_word.shape)\n",
        "for x in x_train_token_ids:\n",
        "    for token in x:\n",
        "        count[token] += 1\n",
        "indices = np.argsort(-count)\n",
        "id_to_word = id_to_word[indices]\n",
        "count = count[indices]\n",
        "\n",
        "hist = np.histogram(count,bins=[1,10,100,1000,10000])\n",
        "print(hist)\n",
        "for i in range(10):\n",
        "    print(id_to_word[i],count[i])\n",
        "\n",
        "## recreate word_to_id based on sorted list\n",
        "word_to_id = {token: (idx+4) for idx, token in enumerate(id_to_word)}\n",
        "\n",
        "# add start/end/unknown token\n",
        "word_to_id['<S>'] = 1\n",
        "word_to_id['</S>'] = 2\n",
        "word_to_id['UNK'] = 3\n",
        "\n",
        "# add start/end/unknown token\n",
        "id_to_word = np.insert(id_to_word, 0, 'UNK')\n",
        "id_to_word = np.insert(id_to_word, 0, '</S>')\n",
        "id_to_word = np.insert(id_to_word, 0, '<S>')\n",
        "id_to_word = np.insert(id_to_word, 0, '<MASK>')\n",
        "\n",
        "'''\n",
        "## assign -1 if token doesn't appear in our dictionary\n",
        "## add +1 to all token ids, we went to reserve id=0 for an unknown token\n",
        "cap_val_token_ids = [[word_to_id.get(token,-1)+1 for token in x] for x in cap_val]\n",
        "cap_train_token_ids = [[word_to_id.get(token,-1)+1 for token in x] for x in cap_train]\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(array([22103,  5084,  1760,   524]), array([    1,    10,   100,  1000, 10000]))\n",
            "a 978638.0\n",
            ". 444424.0\n",
            "on 215658.0\n",
            "of 204059.0\n",
            "the 197760.0\n",
            "in 184003.0\n",
            "with 154800.0\n",
            "and 140762.0\n",
            "is 98209.0\n",
            "man 73322.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n## assign -1 if token doesn't appear in our dictionary\\n## add +1 to all token ids, we went to reserve id=0 for an unknown token\\ncap_val_token_ids = [[word_to_id.get(token,-1)+1 for token in x] for x in cap_val]\\ncap_train_token_ids = [[word_to_id.get(token,-1)+1 for token in x] for x in cap_train]\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrW07PCUAh52",
        "colab_type": "code",
        "outputId": "534448f3-0bda-4a40-d0b5-30865443e717",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "id_to_word[word_to_id.get('dog')]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dog'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dhTcDNPBDwr",
        "colab_type": "code",
        "outputId": "b88b0596-abd6-422c-d60e-93b88ad81b6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "id_to_word[18]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "','"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa0ZFkVMNZrQ",
        "colab_type": "code",
        "outputId": "c1a3980a-3fb1-4fd6-cf96-2e5624939b5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "os.chdir('/content/drive/My Drive/Colab Notebooks/IE534_ImageCaptioning/')\n",
        "\n",
        "# save word_to_id\n",
        "import pickle\n",
        "with open('word_to_id.p', 'wb') as fp:\n",
        "    pickle.dump(word_to_id, fp, protocol=4)\n",
        "\n",
        "## save id_to_word\n",
        "np.save('id_to_word.npy',np.asarray(id_to_word))\n",
        "\n",
        "'''\n",
        "## save training data to single text file\n",
        "with io.open('val_captions.txt','w',encoding='utf-8') as f:\n",
        "    for tokens in cap_val_token_ids:\n",
        "        for token in tokens:\n",
        "            f.write(\"%i \" % token)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "## save test data to single text file\n",
        "with io.open('train_captions.txt','w',encoding='utf-8') as f:\n",
        "    for tokens in cap_train_token_ids:\n",
        "        for token in tokens:\n",
        "            f.write(\"%i \" % token)\n",
        "        f.write(\"\\n\")\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n## save training data to single text file\\nwith io.open(\\'val_captions.txt\\',\\'w\\',encoding=\\'utf-8\\') as f:\\n    for tokens in cap_val_token_ids:\\n        for token in tokens:\\n            f.write(\"%i \" % token)\\n        f.write(\"\\n\")\\n\\n## save test data to single text file\\nwith io.open(\\'train_captions.txt\\',\\'w\\',encoding=\\'utf-8\\') as f:\\n    for tokens in cap_train_token_ids:\\n        for token in tokens:\\n            f.write(\"%i \" % token)\\n        f.write(\"\\n\")\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    }
  ]
}